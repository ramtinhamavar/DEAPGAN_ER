{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c589a158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import keras\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras import backend as K\n",
    "import cv2\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "sess = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6bee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(data_30_sec):\n",
    "    sampling_rate = 128\n",
    "    frame_size = 500\n",
    "    hop_length = 125\n",
    "    max_after_normalize = 1\n",
    "    min_after_normalize = -1\n",
    "    extracted_features = np.zeros(shape=(21, 251, 3))\n",
    "    \n",
    "    data_splited = list()\n",
    "    \n",
    "    data_splited.append(data_30_sec[0:sampling_rate*10])\n",
    "    data_splited.append(data_30_sec[sampling_rate*10:sampling_rate*20])\n",
    "    data_splited.append(data_30_sec[sampling_rate*20:])\n",
    "    data_splited = np.array(data_splited)\n",
    "    \n",
    "    for data_index in range(0, data_splited.shape[0]):\n",
    "        stft = librosa.stft(data_splited[data_index], n_fft=frame_size, hop_length=hop_length)\n",
    "        stft_db = librosa.amplitude_to_db(abs(stft))\n",
    "        norm_stft = (stft_db - stft_db.min())/(stft_db.max() - stft_db.min())\n",
    "        norm_stft = norm_stft*(max_after_normalize - min_after_normalize) + min_after_normalize\n",
    "        \n",
    "        extracted_features[:, :, data_index] = np.transpose(norm_stft)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return extracted_features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_spectrogram(Y, sr, hop_length, y_axis=\"linear\"):\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    librosa.display.specshow(Y,\n",
    "                             sr=sr,\n",
    "                             hop_length=hop_length,\n",
    "                             x_axis=\"time\",\n",
    "                             y_axis=y_axis)\n",
    "    plt.colorbar(format=\"%+2.f\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5da3302",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Augmentation Data ###############\n",
    "\n",
    "def trial_file_processor(file_addr, trial):\n",
    "    \n",
    "    start_points_list = list()\n",
    "    start_offset = 3\n",
    "    end_of_record_starts = 33\n",
    "    window_length = 10\n",
    "    for start_point in range(start_offset, end_of_record_starts, 5):\n",
    "        start_points_list.append([start_point, \n",
    "                                start_point+window_length,\n",
    "                                start_point+2*window_length])\n",
    "    \n",
    "\n",
    "    with open(file_addr, 'rb') as file:\n",
    "        subject_file = pickle.load(file, encoding='latin1')    \n",
    "    \n",
    "    max_after_normalize = 1\n",
    "    min_after_normalize = 0 \n",
    "\n",
    "    color_channel_stft = None\n",
    "    full_norm_stft = np.zeros(shape=(len(start_points_list), 129, 440, 3))\n",
    "    \n",
    "    start_point_number = 0\n",
    "    for start_points in start_points_list:\n",
    "        for color_channel in range(0, 3):\n",
    "            for channel in range(0, 40):\n",
    "                stft = librosa.stft(subject_file[\"data\"][trial, channel,\n",
    "                                              128*(start_points[color_channel]):128*(start_points[color_channel]+window_length)],\n",
    "                                              n_fft=256, hop_length=125)\n",
    "                stft_db = librosa.amplitude_to_db(abs(stft))\n",
    "                norm_stft = (stft_db - stft_db.min())/(stft_db.max() - stft_db.min())\n",
    "                norm_stft = norm_stft*(max_after_normalize - min_after_normalize) + min_after_normalize\n",
    "\n",
    "                if color_channel_stft is None:\n",
    "                    color_channel_stft = norm_stft\n",
    "                else:\n",
    "                    color_channel_stft = np.append(color_channel_stft, norm_stft, axis=1)\n",
    "                    \n",
    "            full_norm_stft[start_point_number, : , :, color_channel] = color_channel_stft\n",
    "            color_channel_stft = None\n",
    "            \n",
    "        start_point_number = start_point_number + 1\n",
    "        \n",
    "    return full_norm_stft, np.array(subject_file[\"labels\"][trial])\n",
    "\n",
    "\n",
    "raw_files_addr = \"Address/to/preprocessed_DEAP_data\"\n",
    "transformed_files_addr = \"Address/to/save/data\"\n",
    "file_names = os.listdir(raw_files_addr)\n",
    "\n",
    "for file_name in file_names:\n",
    "    subject_name = file_name.split(\".\")[0]\n",
    "    for trial in range(0, 40):\n",
    "        data, labels = trial_file_processor(raw_files_addr+file_name, trial)\n",
    "        for sample in range(0, data.shape[0]):\n",
    "            data_label_dict = {\"data\":data[sample], \"labels\":labels}\n",
    "            \n",
    "            with open(transformed_files_addr+subject_name+\"t\"+str(trial)+\"s\"+str(sample), 'wb') as handle:\n",
    "                pickle.dump(data_label_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1814948",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_files_addr = \"Address/to/preprocessed_DEAP_data\"\n",
    "transformed_files_addr = \"A:/Hamavar/DEAP_Dataset/transformed_data/\"\n",
    "file_names = os.listdir(raw_files_addr)\n",
    "\n",
    "def trial_file_processor(file_addr, trial):\n",
    "    \n",
    "    start_points_list = list()\n",
    "    number_of_windows = 3\n",
    "    window_length = 10\n",
    "    for offset in range(0, 30, 2):\n",
    "        start_points_list.append([offset, offset+window_length, offset+2*window_length])\n",
    "\n",
    "\n",
    "    with open(file_addr, 'rb') as file:\n",
    "        subject_file = pickle.load(file, encoding='latin1')    \n",
    "    \n",
    "    max_after_normalize = 1\n",
    "    min_after_normalize = 0 \n",
    "\n",
    "    color_channel_stft = None\n",
    "    full_norm_stft = np.zeros(shape=(len(start_points_list),129, 440, 3))\n",
    "    \n",
    "    start_point_number = 0\n",
    "    for start_points in start_points_list:\n",
    "        for color_channel in range(0, 3):\n",
    "            for channel in range(0, 40):\n",
    "                stft = librosa.stft(subject_file[\"data\"][trial, channel,\n",
    "                                              128*(start_points[color_channel]):128*(start_points[color_channel]+window_length)],\n",
    "                                              n_fft=256, hop_length=125)\n",
    "                stft_db = librosa.amplitude_to_db(abs(stft))\n",
    "                norm_stft = (stft_db - stft_db.min())/(stft_db.max() - stft_db.min())\n",
    "                norm_stft = norm_stft*(max_after_normalize - min_after_normalize) + min_after_normalize\n",
    "\n",
    "                if color_channel_stft is None:\n",
    "                    color_channel_stft = norm_stft\n",
    "                else:\n",
    "                    color_channel_stft = np.append(color_channel_stft, norm_stft, axis=1)\n",
    "                    \n",
    "            full_norm_stft[start_point_number, : , :, color_channel] = color_channel_stft\n",
    "            color_channel_stft = None\n",
    "            \n",
    "        start_point_number = start_point_number + 1\n",
    "        \n",
    "    return full_norm_stft, np.array(subject_file[\"labels\"][trial])\n",
    "\n",
    "\n",
    "dsize = (512, 128)\n",
    "\n",
    "data, labels = trial_file_processor(raw_files_addr+\"s01.dat\", 5)\n",
    "print(data.shape)\n",
    "data1 = cv2.resize(data[0], dsize)\n",
    "data2 = cv2.resize(data[1], dsize)\n",
    "for file_name in file_names:\n",
    "    subject_name = file_name.split(\".\")[0]\n",
    "    for trial in range(0, 40):\n",
    "        data, labels = trial_file_processor(raw_files_addr+file_name, trial)\n",
    "        for sample in range(0, data.shape[0]):\n",
    "            data_label_dict = {\"data\":cv2.resize(data[sample], dsize), \"labels\":labels}\n",
    "            with open(transformed_files_addr+subject_name+\"t\"+str(trial)+\"s\"+str(sample), 'wb') as handle:\n",
    "                pickle.dump(data_label_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f69c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Only EEG Data #################\n",
    "\n",
    "def trial_file_processor(file_addr, trial):\n",
    "    \n",
    "    offset = 3\n",
    "    window_length = 10\n",
    "    start_points = [15+offset, 25+offset, 35+offset]\n",
    "    \n",
    "    with open(file_addr, 'rb') as file:\n",
    "        subject_file = pickle.load(file, encoding='latin1')    \n",
    "    \n",
    "    max_after_normalize = 1\n",
    "    min_after_normalize = 0 \n",
    "\n",
    "    color_channel_stft = None\n",
    "    full_norm_stft = np.zeros(shape=(129, 352, 3))\n",
    "    \n",
    "    \n",
    "    for color_channel in range(0, 2):\n",
    "        for channel in range(0, 32):\n",
    "            stft = librosa.stft(subject_file[\"data\"][trial, channel,\n",
    "                                          128*(start_points[color_channel]):128*(start_points[color_channel]+window_length)],\n",
    "                                          n_fft=256, hop_length=125)\n",
    "            stft_db = librosa.amplitude_to_db(abs(stft))\n",
    "            norm_stft = (stft_db - stft_db.min())/(stft_db.max() - stft_db.min())\n",
    "            norm_stft = norm_stft*(max_after_normalize - min_after_normalize) + min_after_normalize\n",
    "\n",
    "            if color_channel_stft is None:\n",
    "                color_channel_stft = norm_stft\n",
    "            else:\n",
    "                color_channel_stft = np.append(color_channel_stft, norm_stft, axis=1)\n",
    "\n",
    "        full_norm_stft[: , :, color_channel] = color_channel_stft\n",
    "        color_channel_stft = None\n",
    "\n",
    "    return full_norm_stft, subject_file[\"labels\"][trial] \n",
    "\n",
    "\n",
    "raw_files_addr = \"Address/to/preprocessed_DEAP_data\"\n",
    "transformed_files_addr = \"E:/Hamavar/DEAP_Dataset/onlyEEG_transformed_data/\"\n",
    "file_names = os.listdir(raw_files_addr)\n",
    "\n",
    "for file_name in file_names:\n",
    "    for trial in range(0, 40):\n",
    "        data, labels = trial_file_processor(\"Address/to/preprocessed_DEAP_data\"+file_name, trial)\n",
    "        data_label_dict = {\"data\":data, \"labels\":labels}\n",
    "        subject_name = file_name.split(\".\")[0]\n",
    "        with open(transformed_files_addr+subject_name+\"t\"+str(trial), 'wb') as handle:\n",
    "            pickle.dump(data_label_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca405c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Preprocessing Data ################\n",
    "\n",
    "def trial_file_processor(file_addr, trial):\n",
    "    \n",
    "    offset = 3\n",
    "    window_length = 10\n",
    "    start_points = [15+offset, 25+offset, 35+offset]\n",
    "    \n",
    "    with open(file_addr, 'rb') as file:\n",
    "        subject_file = pickle.load(file, encoding='latin1')    \n",
    "    \n",
    "    max_after_normalize = 1\n",
    "    min_after_normalize = -1 \n",
    "\n",
    "    color_channel_stft = None\n",
    "    full_norm_stft = np.zeros(shape=(129, 440, 3))\n",
    "    \n",
    "    \n",
    "    for color_channel in range(0, 2):\n",
    "        for channel in range(0, 40):\n",
    "            stft = librosa.stft(subject_file[\"data\"][trial, channel,\n",
    "                                          128*(start_points[color_channel]):128*(start_points[color_channel]+window_length)],\n",
    "                                          n_fft=256, hop_length=125)\n",
    "            stft_db = librosa.amplitude_to_db(abs(stft))\n",
    "            norm_stft = (stft_db - stft_db.min())/(stft_db.max() - stft_db.min())\n",
    "            norm_stft = norm_stft*(max_after_normalize - min_after_normalize) + min_after_normalize\n",
    "\n",
    "            if color_channel_stft is None:\n",
    "                color_channel_stft = norm_stft\n",
    "            else:\n",
    "                color_channel_stft = np.append(color_channel_stft, norm_stft, axis=1)\n",
    "\n",
    "        full_norm_stft[: , :, color_channel] = color_channel_stft\n",
    "        color_channel_stft = None\n",
    "\n",
    "    return full_norm_stft, subject_file[\"labels\"][trial]\n",
    "\n",
    "\n",
    "raw_files_addr = \"Address/to/preprocessed_DEAP_data\"\n",
    "transformed_files_addr = \"Address/to/save/data\"\n",
    "file_names = os.listdir(raw_files_addr)\n",
    "\n",
    "for file_name in file_names:\n",
    "    for trial in range(0, 40):\n",
    "        data, labels = trial_file_processor(\"Address/to/preprocessed_DEAP_data\"+file_name, trial)\n",
    "        data_label_dict = {\"data\":data, \"labels\":labels}\n",
    "        subject_name = file_name.split(\".\")[0]\n",
    "        with open(transformed_files_addr+subject_name+\"t\"+str(trial), 'wb') as handle:\n",
    "            pickle.dump(data_label_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3f53eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# sample data loading #############\n",
    "transformed_files_addr = \"Address/to/save/data\"\n",
    "with open(transformed_files_addr+\"s01t1s4\", 'rb') as handle:\n",
    "    test = pickle.load(handle)\n",
    "\n",
    "print(test[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccce29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## data generator ################\n",
    "transformed_files_addr = \"Address/to/save/data\"\n",
    "\n",
    "def train_test_spliter(addr, train_ratio, val_ratio):\n",
    "    \n",
    "    files = os.listdir(addr)\n",
    "    number_of_files = len(files)\n",
    "    random.shuffle(files)\n",
    "    end_of_train_files = math.floor(train_ratio*number_of_files)\n",
    "    train_files = files[0:end_of_train_files]\n",
    "    validation_files = files[end_of_train_files:end_of_train_files+math.floor(val_ratio*number_of_files)]\n",
    "    test_files = files[end_of_train_files+math.floor(val_ratio*number_of_files):]\n",
    "    \n",
    "    return train_files, validation_files, test_files\n",
    "    \n",
    "def OLO_train_test_spliter(addr, val_one, test_one):\n",
    "    # val_one and test_one should be between 1 and 32\n",
    "    files = os.listdir(addr)\n",
    "    \n",
    "    \n",
    "    train_files = files\n",
    "    \n",
    "    val_files = list()\n",
    "    val_re_string = None\n",
    "    \n",
    "    test_files = list()\n",
    "    test_re_string = None\n",
    "    \n",
    "    if val_one < 10:\n",
    "        val_re_string = f\"s0{val_one}.+\"\n",
    "    else:\n",
    "        val_re_string = f\"s{val_one}.+\"\n",
    "        \n",
    "    if test_one < 10:\n",
    "        test_re_string = f\"s0{test_one}.+\"\n",
    "    else:\n",
    "        test_re_string = f\"s{test_one}.+\"\n",
    "    \n",
    "    for file in files:\n",
    "        if re.search(val_re_string, file):\n",
    "            val_files.append(file)\n",
    "        \n",
    "        if re.search(test_re_string, file):\n",
    "            test_files.append(file)\n",
    "            \n",
    "    \n",
    "    for index in range(0, len(val_files)):\n",
    "            train_files.remove(val_files[index])\n",
    "            train_files.remove(test_files[index])\n",
    "    \n",
    "    return train_files, val_files, test_files\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "class ERDataGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, files_addrs, feature_files_addr, batch_size=32):\n",
    "        self.files_addrs = files_addrs\n",
    "        self.feature_files_addr = feature_files_addr\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        \n",
    "    def number_of_samples(self):\n",
    "        return len(self.files_addrs)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.files_addrs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_files_addrs = self.files_addrs[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        batch_data, batch_labels = self.__data_generation(batch_files_addrs)\n",
    "\n",
    "        return batch_data, batch_labels\n",
    "\n",
    "    \n",
    "\n",
    "    def __data_generation(self, batch_file_addrs):\n",
    "        batch_data = list()\n",
    "        batch_labels = list()\n",
    "\n",
    "        for batch_file_addr in batch_file_addrs:\n",
    "            with open(self.feature_files_addr+batch_file_addr, 'rb') as handle:\n",
    "                loaded_data = pickle.load(handle)\n",
    "                batch_data.append(loaded_data[\"data\"])\n",
    "                batch_labels.append(loaded_data[\"labels\"])\n",
    "\n",
    "        return (np.array(batch_data), np.array(batch_labels)) \n",
    "    \n",
    "    \n",
    "    \n",
    "batch_size = 32    \n",
    "train_files, validation_files, test_files = OLO_train_test_spliter(transformed_files_addr,1, 2)\n",
    "\n",
    "\n",
    "test_samples = len(test_files)\n",
    "train_gen = ERDataGenerator(train_files, transformed_files_addr)\n",
    "validation_gen = ERDataGenerator(validation_files, transformed_files_addr)\n",
    "test_gen = ERDataGenerator(test_files, transformed_files_addr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9507cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ER_OCC_DataGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, files_addrs, feature_files_addr, o_channels_freqs_pairs, batch_size=32):\n",
    "        \n",
    "        self._channel_length = 11\n",
    "        self._freq_bands = [[0, 129]]\n",
    "        \n",
    "        self.o_channels_freqs_pairs = o_channels_freqs_pairs\n",
    "        \n",
    "        self.files_addrs = files_addrs\n",
    "        self.feature_files_addr = feature_files_addr\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    \n",
    "    def number_of_samples(self):\n",
    "        return len(self.files_addrs)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.files_addrs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_files_addrs = self.files_addrs[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        batch_data, batch_labels = self.__data_generation(batch_files_addrs)\n",
    "\n",
    "        return batch_data, batch_labels\n",
    "\n",
    "    \n",
    "    def __data_generation(self, batch_file_addrs):\n",
    "        batch_data = list()\n",
    "        batch_labels = list()\n",
    "    \n",
    "        for batch_file_addr in batch_file_addrs:\n",
    "            with open(self.feature_files_addr+batch_file_addr, 'rb') as handle:\n",
    "                loaded_data = pickle.load(handle)\n",
    "                batch_data.append(loaded_data[\"data\"])\n",
    "                batch_labels.append(loaded_data[\"labels\"])\n",
    "        \n",
    "        occ_data = self._occ(np.array(batch_data), self.o_channels_freqs_pairs)\n",
    "        return (occ_data, np.array(batch_labels)) \n",
    "    \n",
    "    def _occ(self, data, o_channels_freq_pairs):\n",
    "        masked_data = np.zeros(shape=(data.shape[0], data.shape[1], data.shape[2], data.shape[3]))\n",
    "        occ_mask_1D = np.ones(shape=(data.shape[1], data.shape[2]))\n",
    "        occ_mask = np.zeros(shape=(data.shape[1], data.shape[2], data.shape[3]))\n",
    "        for channel_freq_pair in o_channels_freq_pairs:\n",
    "            channel = channel_freq_pair[0]\n",
    "            freq_band = channel_freq_pair[1]\n",
    "            low_freq = self._freq_bands[freq_band][0]\n",
    "            high_freq = self._freq_bands[freq_band][1]\n",
    "            occ_mask_1D[low_freq:high_freq, channel*self._channel_length:(channel+1)*self._channel_length] = 0\n",
    "        \n",
    "        for color_channel in range(0, data.shape[3]):\n",
    "            occ_mask[:, :, color_channel] = occ_mask_1D\n",
    "        \n",
    "        for sample in range(0, data.shape[0]):\n",
    "            masked_data[sample, :, :, :] = data[sample, :, :, :]*occ_mask \n",
    "            \n",
    "        return masked_data\n",
    "        \n",
    "        \n",
    "\n",
    "occ_gen = ER_OCC_DataGenerator(test_files, transformed_files_addr, [[2, 0], [5, 0], [8, 0], [12, 0], [14, 0], [16, 0],\n",
    "                                                                    [18, 0], [19, 0], [23, 0], [25, 0], [28, 0], [32, 0], [33, 0], [35, 0],\n",
    "                                                                    [36, 0]], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c59d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = keras.Input(shape=(129, 440, 3))\n",
    "\n",
    "x = tf.keras.layers.Conv2D(16, kernel_size=(5, 11), padding=\"same\", activation=\"relu\")(input_layer)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = tf.keras.layers.Dropout(rate=0.2)(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(8, kernel_size=(3, 3), padding=\"same\",activation=\"relu\")(x)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = tf.keras.layers.Dropout(rate=0.2)(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(4, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")(x)\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = tf.keras.layers.Dropout(rate=0.2)(x)\n",
    "\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dropout(rate=0.1)(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(units=512, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dropout(rate=0.1)(x)\n",
    "x = tf.keras.layers.Dense(units=256, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dropout(rate=0.1)(x)\n",
    "x = tf.keras.layers.Dense(units=128, activation=\"relu\")(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(units=4)(x)\n",
    "\n",
    "Model = tf.keras.models.Model(input_layer, x)\n",
    "Model.compile(loss='mae', optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0005)) # tf.keras.optimizers.Adam(learning_rate=0.007)\n",
    "\n",
    "Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49499254",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hist = Model.fit(train_gen, validation_data=validation_gen, epochs=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded72e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_epochs = 200\n",
    "train_mae = np.zeros(shape=(number_of_epochs, 1))\n",
    "val_mae = np.zeros(shape=(number_of_epochs, 1))\n",
    "\n",
    "for epoch in range(0, number_of_epochs):\n",
    "    print(f\"epoch number:{epoch+1}\")\n",
    "    K.clear_session()\n",
    "    train_hist = Model.fit(train_gen, steps_per_epoch=steps_per_epoch)\n",
    "    train_mae[epoch, 0] = train_hist.history['loss'][0] \n",
    "    val_data, val_labels = next(validation_gen)\n",
    "    test_hist = Model.evaluate(val_data, val_labels)\n",
    "    val_mae[epoch, 0] = test_hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c15d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_hist.history['loss'])\n",
    "plt.plot(train_hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaa45e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values = np.zeros(shape=(test_samples, 4))\n",
    "test_labels = np.zeros(shape=(test_samples, 4))\n",
    "\n",
    "counter = 0\n",
    "for data, labels in test_gen:\n",
    "    predicted = Model.predict(data)\n",
    "    \n",
    "    predicted_values[counter*batch_size:(counter+1)*batch_size, :] = predicted\n",
    "    test_labels[counter*batch_size:(counter+1)*batch_size, :] = labels\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd2723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values = np.zeros(shape=(test_samples, 4))\n",
    "test_labels = np.zeros(shape=(test_samples, 4))\n",
    "\n",
    "counter = 0\n",
    "for data, labels in occ_gen:\n",
    "    predicted = Model.predict(data)\n",
    "    \n",
    "    predicted_values[counter*batch_size:(counter+1)*batch_size, :] = predicted\n",
    "    test_labels[counter*batch_size:(counter+1)*batch_size, :] = labels\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4298bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_evaluation_percentage(true_labels, predicted_labels):\n",
    "    labels_dimension = 4\n",
    "    false_predictions = np.zeros(shape=(1, labels_dimension))\n",
    "    total_numbers_of_test_samples = len(predicted_labels)\n",
    "    \n",
    "    for prd_idx in range(0, total_numbers_of_test_samples):\n",
    "        for label_idx in range(0, labels_dimension):\n",
    "            if ((predicted_labels[prd_idx, label_idx]>5) and (true_labels[prd_idx, label_idx]<5)) or \\\n",
    "            ((predicted_labels[prd_idx, label_idx]<5) and (true_labels[prd_idx, label_idx]>5)):\n",
    "                false_predictions[0, label_idx] = false_predictions[0, label_idx] + 1\n",
    "\n",
    "    return ((total_numbers_of_test_samples - false_predictions)/total_numbers_of_test_samples)*100\n",
    "\n",
    "\n",
    "\n",
    "def predict_labels(Model, generator):\n",
    "    predicted_labels = np.zeros(shape=(generator.number_of_samples(), 4))\n",
    "    true_labels = np.zeros(shape=(generator.number_of_samples(), 4))\n",
    "\n",
    "    counter = 0\n",
    "    for data, labels in generator:\n",
    "        predicted = Model.predict(data)\n",
    "    \n",
    "        predicted_labels[counter*batch_size:(counter+1)*batch_size, :] = predicted\n",
    "        true_labels[counter*batch_size:(counter+1)*batch_size, :] = labels\n",
    "        counter = counter + 1\n",
    "        \n",
    "    return true_labels, predicted_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c6f7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the normal loss values\n",
    "number_of_channels = 40\n",
    "\n",
    "\n",
    "true_labels, predicted_labels = predict_labels(Model, test_gen)  \n",
    "ground_truth = accuracy_evaluation_percentage(true_labels, predicted_labels)\n",
    "\n",
    "channel_freq_occ_loss = np.zeros(shape=(9, number_of_channels, 4))\n",
    "for channel_index in range(0, 40):\n",
    "    for freq_index in range(0, 1):\n",
    "        occ_gen = ER_OCC_DataGenerator(test_files, transformed_files_addr, [[channel_index, freq_index]], batch_size=32)\n",
    "        \n",
    "        true_labels, predicted_labels = predict_labels(Model, occ_gen)\n",
    "        channel_freq_occ_loss[freq_index, channel_index, :] = ground_truth - accuracy_evaluation_percentage(true_labels, predicted_labels)\n",
    "    \n",
    "\n",
    "reshape_loss_values = channel_freq_occ_loss.reshape((channel_freq_occ_loss.shape[0]*channel_freq_occ_loss.shape[1], channel_freq_occ_loss.shape[2])) \n",
    "    \n",
    "for feeling_index in range(0, 3):\n",
    "    reshape_loss_values[:, feeling_index] = (reshape_loss_values[:, feeling_index] - reshape_loss_values[:, feeling_index].min())/(reshape_loss_values[:, feeling_index].max() - reshape_loss_values[:, feeling_index].min())\n",
    "    \n",
    "norm_loss_values = reshape_loss_values.reshape((channel_freq_occ_loss.shape[0], channel_freq_occ_loss.shape[1], channel_freq_occ_loss.shape[2]))\n",
    "norm_loss_values = norm_loss_values*255\n",
    "print(norm_loss_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0930201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.evaluate(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6aa74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_length = 11\n",
    "freq_height = 129\n",
    "freq_bands = [[0, 129]]#[[0, 4], [4, 8], [8, 14], [14, 30], [30, 50], [50, 70], [70, 90], [90, 110], [110, 129]]\n",
    "\n",
    "channel_heat_map = np.ones(shape=(freq_height, channel_length))\n",
    "heat_map = np.zeros(shape=(freq_height, 40*channel_length, 3))\n",
    "for frequency_index in range(0, len(freq_bands)):\n",
    "    for channel_index in range(0, 40):\n",
    "        for color_index in range(0, 3):\n",
    "            heat_map[freq_bands[frequency_index][0]:freq_bands[frequency_index][1], channel_index*channel_length:(channel_index+1)*channel_length, color_index] = \\\n",
    "            channel_heat_map[freq_bands[frequency_index][0]:freq_bands[frequency_index][1],:]*norm_loss_values[frequency_index, channel_index, color_index]\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7bad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_evaluation_percentage(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8f8dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### SC-Preprocessing Data ################\n",
    "\n",
    "def trial_file_processor(file_addr, trial):\n",
    "    \n",
    "    offset = 3\n",
    "    window_length = 10\n",
    "    start_points = [15+offset, 25+offset, 35+offset]\n",
    "    selected_channels = np.array([ 1, 2, 4, 7, 8, 11, 12, 13, 15, 16, 17, 18, 19, 22, 27, 28, 30, 31, 32, 33, 37, 38])\n",
    " \n",
    "    \n",
    "    with open(file_addr, 'rb') as file:\n",
    "        subject_file = pickle.load(file, encoding='latin1')    \n",
    "    \n",
    "    max_after_normalize = 1\n",
    "    min_after_normalize = -1 \n",
    "\n",
    "    color_channel_stft = None\n",
    "    full_norm_stft = np.zeros(shape=(129, 242, 3))\n",
    "    \n",
    "    \n",
    "    for color_channel in range(0, 2):\n",
    "        for channel in selected_channels:\n",
    "            stft = librosa.stft(subject_file[\"data\"][trial, channel,\n",
    "                                          128*(start_points[color_channel]):128*(start_points[color_channel]+window_length)],\n",
    "                                          n_fft=256, hop_length=125)\n",
    "            stft_db = librosa.amplitude_to_db(abs(stft))\n",
    "            norm_stft = (stft_db - stft_db.min())/(stft_db.max() - stft_db.min())\n",
    "            norm_stft = norm_stft*(max_after_normalize - min_after_normalize) + min_after_normalize\n",
    "\n",
    "            if color_channel_stft is None:\n",
    "                color_channel_stft = norm_stft\n",
    "            else:\n",
    "                color_channel_stft = np.append(color_channel_stft, norm_stft, axis=1)\n",
    "\n",
    "        full_norm_stft[: , :, color_channel] = color_channel_stft\n",
    "        color_channel_stft = None\n",
    "\n",
    "    return full_norm_stft, subject_file[\"labels\"][trial]\n",
    "\n",
    "\n",
    "raw_files_addr = \"Address/to/preprocessed_DEAP_data\"\n",
    "transformed_files_addr = \"E:/Hamavar/DEAP_Dataset/selectedChannelsData/median/\"\n",
    "file_names = os.listdir(raw_files_addr)\n",
    "\n",
    "for file_name in file_names:\n",
    "    for trial in range(0, 40):\n",
    "        data, labels = trial_file_processor(\"Address/to/preprocessed_DEAP_data\"+file_name, trial)\n",
    "        data_label_dict = {\"data\":data, \"labels\":labels}\n",
    "        subject_name = file_name.split(\".\")[0]\n",
    "        with open(transformed_files_addr+subject_name+\"t\"+str(trial), 'wb') as handle:\n",
    "            pickle.dump(data_label_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
