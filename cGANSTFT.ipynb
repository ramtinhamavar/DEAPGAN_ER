{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d466ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras import backend as K\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cdcf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_file_processor(file_addr, trial):\n",
    "    \n",
    "    start_points_list = list()\n",
    "    start_offset = 3\n",
    "    end_of_record_starts = 33\n",
    "    window_length = 10\n",
    "    for start_point in range(start_offset, end_of_record_starts, 5):\n",
    "        start_points_list.append([start_point, \n",
    "                                start_point+window_length,\n",
    "                                start_point+2*window_length])\n",
    "    \n",
    "\n",
    "    with open(file_addr, 'rb') as file:\n",
    "        subject_file = pickle.load(file, encoding='latin1')    \n",
    "    \n",
    "    max_after_normalize = 1\n",
    "    min_after_normalize = -1 \n",
    "\n",
    "    color_channel_stft = None\n",
    "    full_norm_stft = np.zeros(shape=(len(start_points_list), 129, 440, 3))\n",
    "    \n",
    "    start_point_number = 0\n",
    "    for start_points in start_points_list:\n",
    "        for color_channel in range(0, 3):\n",
    "            for channel in range(0, 40):\n",
    "                stft = librosa.stft(subject_file[\"data\"][trial, channel,\n",
    "                                              128*(start_points[color_channel]):128*(start_points[color_channel]+window_length)],\n",
    "                                              n_fft=256, hop_length=125)\n",
    "                stft_db = librosa.amplitude_to_db(abs(stft))\n",
    "                norm_stft = (stft_db - stft_db.min())/(stft_db.max() - stft_db.min())\n",
    "                norm_stft = norm_stft*(max_after_normalize - min_after_normalize) + min_after_normalize\n",
    "\n",
    "                if color_channel_stft is None:\n",
    "                    color_channel_stft = norm_stft\n",
    "                else:\n",
    "                    color_channel_stft = np.append(color_channel_stft, norm_stft, axis=1)\n",
    "                    \n",
    "            full_norm_stft[start_point_number, : , :, color_channel] = color_channel_stft\n",
    "            color_channel_stft = None\n",
    "            \n",
    "        start_point_number = start_point_number + 1\n",
    "        \n",
    "    return full_norm_stft, np.array(subject_file[\"labels\"][trial])\n",
    "\n",
    "\n",
    "raw_files_addr = \"E:/Hamavar/DEAP_Dataset/data_preprocessed_python/\"\n",
    "transformed_files_addr = \"A:/Hamavar/DEAP_Dataset/augmented_transformed_data_resized/\"\n",
    "file_names = os.listdir(raw_files_addr)\n",
    "\n",
    "dsize = (512, 128)\n",
    "\n",
    "for file_name in file_names:\n",
    "    subject_name = file_name.split(\".\")[0]\n",
    "    for trial in range(0, 40):\n",
    "        data, labels = trial_file_processor(raw_files_addr+file_name, trial) \n",
    "        for sample in range(0, data.shape[0]):\n",
    "            data_label_dict = {\"data\":cv2.resize(data[sample], dsize), \"labels\":labels}\n",
    "            \n",
    "            with open(transformed_files_addr+subject_name+\"t\"+str(trial)+\"s\"+str(sample), 'wb') as handle:\n",
    "                pickle.dump(data_label_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d2e4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_files_addr = \"A:/Hamavar/DEAP_Dataset/augmented_transformed_data_resized/\"\n",
    "with open(transformed_files_addr+\"s01t1s4\", 'rb') as handle:\n",
    "    test = pickle.load(handle)\n",
    "\n",
    "plt.imshow(test[\"data\"])\n",
    "print(test[\"data\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2703f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Disc_Model(input_shape=(128, 512, 3), n_classes=10):\n",
    "    input_label = tf.keras.layers.Input(shape=(1, ))\n",
    "    embedded_label = tf.keras.layers.Embedding(n_classes, 50)(input_label)\n",
    "    \n",
    "    n_nodes = input_shape[0]*input_shape[1]\n",
    "    \n",
    "    embedded_label = tf.keras.layers.Dense(n_nodes)(embedded_label)\n",
    "    embedded_label = tf.keras.layers.Reshape((input_shape[0], input_shape[1], 1))(embedded_label)\n",
    "    \n",
    "    input_img = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    merge = tf.keras.layers.Concatenate()([input_img, embedded_label])\n",
    "    \n",
    "    disc = tf.keras.layers.Conv2D(128, kernel_size=(3, 3), strides=(2, 2), padding=\"same\")(merge)\n",
    "    disc = tf.keras.layers.LeakyReLU(alpha=0.2)(disc)\n",
    "    \n",
    "    disc = tf.keras.layers.Conv2D(128, kernel_size=(3, 3), strides=(2, 2), padding=\"same\")(disc)\n",
    "    disc = tf.keras.layers.LeakyReLU(alpha=0.2)(disc)\n",
    "    \n",
    "    disc = tf.keras.layers.Flatten()(disc)\n",
    "    disc = tf.keras.layers.Dropout(0.4)(disc)\n",
    "    disc = tf.keras.layers.Dense(1, activation=\"sigmoid\")(disc)\n",
    "    \n",
    "    model = tf.keras.models.Model([input_img, input_label], disc)\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def Generator_Model(latent_dim, n_classes=10):\n",
    "    \n",
    "    input_label = tf.keras.layers.Input(shape=(1, ))\n",
    "    input_seed = tf.keras.layers.Embedding(n_classes, 50)(input_label)\n",
    "    \n",
    "    n_nodes = 8*32\n",
    "    \n",
    "    input_seed = tf.keras.layers.Dense(n_nodes)(input_seed)\n",
    "    input_seed = tf.keras.layers.Reshape((8, 32, 1))(input_seed)\n",
    "    \n",
    "    input_latent = tf.keras.layers.Input(shape=(latent_dim,))\n",
    "    \n",
    "    n_nodes = 128*8*32\n",
    "    \n",
    "    gen = tf.keras.layers.Dense(n_nodes)(input_latent)\n",
    "    gen = tf.keras.layers.LeakyReLU(alpha=0.2)(gen)\n",
    "    gen = tf.keras.layers.Reshape((8, 32, 128))(gen)\n",
    "    \n",
    "    merge = tf.keras.layers.Concatenate()([gen, input_seed])\n",
    "    \n",
    "    gen = tf.keras.layers.Conv2DTranspose(128, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")(merge)\n",
    "    gen = tf.keras.layers.LeakyReLU(alpha=0.2)(gen)\n",
    "    \n",
    "    gen = tf.keras.layers.Conv2DTranspose(128, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")(gen)\n",
    "    gen = tf.keras.layers.LeakyReLU(alpha=0.2)(gen)\n",
    "    \n",
    "    gen = tf.keras.layers.Conv2DTranspose(128, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")(gen)\n",
    "    gen = tf.keras.layers.LeakyReLU(alpha=0.2)(gen)\n",
    "    \n",
    "    gen = tf.keras.layers.Conv2DTranspose(128, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")(gen)\n",
    "    gen = tf.keras.layers.LeakyReLU(alpha=0.2)(gen)\n",
    "    \n",
    "    output_layer = tf.keras.layers.Conv2D(3, kernel_size=(7, 7), activation='tanh', padding=\"same\")(gen)\n",
    "    \n",
    "    model = tf.keras.models.Model([input_latent, input_label], output_layer)\n",
    "    \n",
    "    return model\n",
    "  \n",
    "    \n",
    "def GAN_Model(gen_model, disc_model):\n",
    "    disc_model.trainable = False\n",
    "    \n",
    "    gen_noise, gen_label = gen_model.input\n",
    "    gen_output = gen_model.output\n",
    "    \n",
    "    gan_output = disc_model([gen_output, gen_label])\n",
    "    \n",
    "    GAN = tf.keras.models.Model([gen_noise, gen_label], gan_output)\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n",
    "    GAN.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    \n",
    "    return GAN\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505d59c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_files_addr = \"A:/Hamavar/DEAP_Dataset/augmented_transformed_data_resized/\"\n",
    "dataset_addr = os.listdir(transformed_files_addr)\n",
    "\n",
    "\n",
    "def generate_real_samples(dataset_addr, n_samples):\n",
    "    X = list()\n",
    "    labels = list()\n",
    "    \n",
    "    rand_idx = np.random.randint(0, len(dataset_addr), n_samples)\n",
    "    for idx in rand_idx:\n",
    "        with open(transformed_files_addr+dataset_addr[idx], 'rb') as handle:\n",
    "            file_info = pickle.load(handle)\n",
    "            X.append(file_info[\"data\"])\n",
    "            labels.append(int(file_info[\"labels\"][0]))\n",
    "            \n",
    "    y = np.ones((n_samples, 1))\n",
    "    \n",
    "    return [np.array(X), np.array(labels)], y\n",
    "\n",
    "\n",
    "def generate_latent_points(latent_dim, n_samples, n_classes=10):\n",
    "    x_input = np.random.randn(latent_dim * n_samples)\n",
    "    z_input = x_input.reshape(n_samples, latent_dim)\n",
    "    labels = np.random.randint(0, n_classes, n_samples)\n",
    "    \n",
    "    return [z_input, labels]\n",
    "\n",
    "\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    z_input, labels_input = generate_latent_points(latent_dim, n_samples)\n",
    "    images = generator.predict([z_input, labels_input])\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    \n",
    "    return [images, labels_input], y\n",
    "\n",
    "\n",
    "def generate_images(generator, latent_dim, n_samples, epoch):\n",
    "    z_input, labels_input = generate_latent_points(latent_dim, n_samples)\n",
    "    images = generator.predict([z_input, labels_input])\n",
    "    \n",
    "    for img_idx in range(0, n_samples):\n",
    "        plt.subplot(2,2, img_idx+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow((images[img_idx]+1)/2)\n",
    "    \n",
    "    filename1 = f\"A:/Hamavar/DEAP_Dataset/cGAN/generated_img/plot_{epoch}.png\"\n",
    "    plt.savefig(filename1)\n",
    "    plt.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3817725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g_model, d_model, gan_model, dataset_addr, latent_dim, n_epochs=100, n_batch=32):\n",
    "    bat_per_epo = int(len(dataset_addr) / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        for j in range(2*bat_per_epo):\n",
    "            \n",
    "            [X_real, labels_real], y_real = generate_real_samples(dataset_addr, half_batch)\n",
    "            d_loss1, _ = d_model.train_on_batch([X_real, labels_real], y_real)\n",
    "            \n",
    "            [X_fake, labels], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            d_loss2, _ = d_model.train_on_batch([X_fake, labels], y_fake)\n",
    "            \n",
    "            [z_input, labels_input] = generate_latent_points(latent_dim, n_batch)\n",
    "            y_gan = np.ones((n_batch, 1))\n",
    "            g_loss = gan_model.train_on_batch([z_input, labels_input], y_gan)\n",
    "        \n",
    "        generate_images(g_model, latent_dim, 4, i+1)\n",
    "        print(f\"Epoch : {i+1}, D_Loss : {(d_loss1+d_loss2)/2:.2f}, G_Loss : {g_loss:.2f}\")\n",
    "    g_model.save('A:/Hamavar/DEAP_Dataset/cGAN/Model/cgan_generator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc28860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "\n",
    "d_model = Disc_Model()\n",
    "g_model = Generator_Model(latent_dim)\n",
    "gan_model = GAN_Model(g_model, d_model)\n",
    "\n",
    "train(g_model, d_model, gan_model, dataset_addr, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d53f55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
